# SocialAmbiance-speaker-count
A Pytorch based concurrent speaker countalgorithm

Introduction: AmbianceCount is an automatic and objective method that extract social ambiance from unconstrained audio recordings by estimating the number of concurrent speakers. AmbianceCount consists of a supervised deep neural network (DNN) embedding extractor to differentiate speech mixtures, and a scoring system for estimation and improving generalization. The performance of AmbianceCount is compared with baseline and evaluated on several synthesized datasets. Lastly, I utilize AmbianceCount to evaluate data from a sociability pilot, with audio data from depression and psychosis patients as well as age-matched healthy controls. Our analysis shows that extracted social ambiance patterns are significantly different across three groups. Besides, it is observe that captured social ambiance patterns are associated with psychometric and personality scores, which is consistent with clinical diagnosis.

**How to use AmbianceCount**

Step 1: Remove silent segments using pre-trained Voice Activity Detection: https://github.com/ina-foss/inaSpeechSegmenter
Suppose that audio recordings of participant_id is stored in /dataroot/Baylor/id, the first step is to remove all the silent segments. 
   | runSegment.sh
   |---- 2h.py // just to prevent stackoverflow
   |---- inaSpeechSegmenter toolkit
   
Step 2: Prepare 5s segments to feed into neural network
   | preparation.sh
   |---- 5s.py 
   
Step 3: Kaldi-format Feature extaction 
Acoustic features are extracted using Kaldi toolkit, and deep embedding features are extracted using trained neural networks
   | toKaldiFormat.sh
   |---- model.py  // models are defined based on https://github.com/Snowdar/asv-subtools/blob/master/pytorch/model/resnet-xvector.py
   
Step 4: Back-end scoring and domain adaptation
Extracted embeddings are scored and adapted to generalize the algorithm using Kaldi
   | backend.sh
   |---- score.py
   
![image](https://user-images.githubusercontent.com/41505580/130548488-82207ef3-c93a-40b9-b4ad-15c461fc6a9d.png)

Step 5: Statistical analysis 
Results: 
1. Social Ambiance Measure (SAM) Differences Between Groups
Figure 2  illustrates that social ambiance patterns extracted from participants with depressive or psychotic disorders were significantly different from healthy controls. 
![Figure2](https://user-images.githubusercontent.com/41505580/130549094-10d4ebfc-d725-46ff-818b-72137d8fc1a4.jpeg)


2. Relationship Between Social Ambiance Measure (SAM) and Self-Reported Measures
Social ambiance patterns, while linked to some personality traits for healthy controls, were found associated with psychometric scores for participants with depressive or psychotic disorders.
![table](https://user-images.githubusercontent.com/41505580/130549103-c2e5f091-42d7-4640-9c4b-a2d5865e8f45.jpeg)

For more details, please refer to our paper:
_Chen W, Sabharwal A, Taylor E, Patel AB and Moukaddam N (2021) Privacy-Preserving Social Ambiance Measure From Free-Living Speech Associates With Chronic Depressive and Psychotic Disorders. Front. Psychiatry 12:670020. doi: 10.3389/fpsyt.2021.670020_

**How to prepare training data**

Overlapped speech creation:
We utilize LibriSpeech corpus to create overlapped speech. By combining three LibriSpeech subsets, clean-360, clean-100 and other-500, I get 960 hours of speech in utterances from 2338 speakers (1228 female speakers and 1210 male speakers), sampled at 16 kHz. Utterances were segmented when the silence intervals were longer than 0.3 seconds or coincided with sentence breaks in the reference text. 

![image](https://user-images.githubusercontent.com/41505580/130540735-6d580b1f-081d-4443-a7bd-e57a9f6918a3.png)

For each speaker, a 15-30 min recording is generated by concatenating LibriSpeech utterances of that speaker. Next, an audio segment is randomly selected from each recording, and segments are randomly adjusted in volume and speed to simulate how people speak in real-world scenarios.

![image](https://user-images.githubusercontent.com/41505580/130540880-791bc03f-176f-4b64-bb5e-8c0411e67fec.png)

Finally, adjusted segments from K speakers are trimmed to T seconds and overlapped with each other to generate a speech mixture, labelled with speaker count of K.

Scenarios creation:
To cover different acoustic scenarios, I add three categories of sound effects: background noises, foreground noises and reverberation using Kaldi toolkit.
![image](https://user-images.githubusercontent.com/41505580/130540984-8c62afca-54d8-4a1a-b0c5-2b111c37f13f.png)

For more details about model training, please refer to my thesis:
_Chen, Wenwan. AmbianceCount: An Objective Social Ambiance Measure from Unconstrained Day-long Audio Recordings. Diss. Rice University, 2020._
